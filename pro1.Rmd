---
title: "PROJECT1"
author: "Christian, O'Neal, Mohammad"
date: "2023-05-30"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
# Required libraries
library(tidyverse) 
library(caret) 
library(leaps)
library(randomForest)
library(corrplot)
```

```{r}
# Load the data
car_data <- read.csv("car_dataset.csv")
```

# OBJECTIVE 1: Display the ability to build regression model

```{r}
# Initial exploratory data analysis
summary(car_data)
pairs(car_data[sapply(car_data, is.numeric)])
```

```{r}
# Correlation analysis
cor_matrix <- cor(car_data[sapply(car_data, is.numeric)])
corrplot(cor_matrix)
```
Feature Engineering
```{r}
# Convert categorical variables to factors
car_data$Make <- as.factor(car_data$Make)
car_data$Model <- as.factor(car_data$Model)
# ...do this for all other categorical variables as necessary
```

Build a Linear Regression Model:
```{r}
model1 <- lm(MSRP ~ ., data = car_data)
```

Evaluate and interprete model
```{r}
summary(model1)
confint(model1)  # Confidence intervals
```
Check Assumptions for linear regression
```{r}
# Diagnostic plots
par(mfrow = c(2, 2))
plot(model1)
```

Perform Exploratory Data Analysis (EDA):
```{r}
# For numeric variables
pairs(car_data[sapply(car_data, is.numeric)])

# For categorical variables, use boxplot or similar
boxplot(MSRP ~ Make, data = car_data)  # Repeat for other categorical variables
```


# Objective 2: Building a More Complex Model for Better Predictions
Add Complexity to Your Model:
```{r}
# Adding interaction term
model2 <- lm(MSRP ~ . + Engine.HP:Engine.Cylinders, data = car_data)  # Just an example, add interactions that make sense

# Adding polynomial term
model3 <- lm(MSRP ~ . + I(Engine.HP^2), data = car_data)  # Add polynomials that make sense
```

Use Cross-Validation or Train/Validation Split:
```{r}
# Split the data
set.seed(123)
train_indices <- sample(1:nrow(car_data), nrow(car_data)*0.7)
train_data <- car_data[train_indices, ]
test_data <- car_data[-train_indices, ]
```



Converting character columns
```{r}
train_data[] <- lapply(train_data, function(x) {
  if (is.character(x)) factor(x) else x
})

test_data[] <- lapply(test_data, function(x) {
  if (is.character(x)) factor(x) else x
})
```

Build a Nonparametric model
```{r}
# Load the library
library(gbm)

# Fit the Gradient Boosting model
set.seed(123)
gbm_model <- gbm(MSRP ~ ., data = train_data,
                 distribution = "gaussian",
                 n.trees = 5000,
                 interaction.depth = 4,
                 shrinkage = 0.01,
                 cv.folds = 5, # 5-fold cross-validation
                 n.minobsinnode = 10)

# Make predictions
gbm_pred <- predict(gbm_model, newdata = test_data, n.trees = gbm_model$gbm.call$best.trees)

# Compute MSE for the GBM model
gbm_mse <- mean((test_data$MSRP - gbm_pred)^2)

# Display MSE
gbm_mse


```

The Gradient Boosting Model (gbm_model) has been fitted on the training data and used to make predictions on the test data. The parameters used in the model, such as the number of trees (5000), the depth of tree interactions (4), the learning rate or shrinkage (0.01), and the minimum observations in node (10), were set according to the problem at hand or based on some heuristic or experience. Also, 5-fold cross-validation was used to estimate the performance of the model.

However, the Mean Squared Error (MSE) for this model turned out to be 3728531510, which is quite large. Remember, MSE is a measure of the average of the squares of the errorsâ€”that is, the average squared difference between the estimated values and the actual value. A lower MSE indicates a better fit of the model to the data.

Given the large MSE, it suggests that the gbm_model did not perform well on the test data. This could be due to several reasons such as overfitting, where the model learns the training data too well and performs poorly on new, unseen data; or it could be that the hyperparameters of the model are not well-tuned and could use some adjustment.

In this case, the model could be improved by either adjusting the hyperparameters (e.g., decreasing the number of trees, adjusting the interaction depth, or changing the learning rate) or by using a different modeling approach. It's also worth noting that GBM models can be sensitive to noisy data and outliers, so preprocessing steps like outlier removal, feature selection, and feature engineering might also improve the model's performance.

Lastly, it's also possible that a different type of model might be more appropriate for this dataset or problem. It's often a good idea to try out several different models and choose the one that performs the best according to your chosen evaluation metric and problem constraints.

Compare MSE's
```{r}
# Compute MSE for the models
pred1 <- predict(model1, newdata = test_data)
mse1 <- mean((test_data$MSRP - pred1)^2)

pred2 <- predict(model2, newdata = test_data)
mse2 <- mean((test_data$MSRP - pred2)^2)

# Add predictions for GBM model
gbm_pred <- predict(gbm_model, newdata = test_data, n.trees = gbm_model$gbm.call$best.trees)
gbm_mse <- mean((test_data$MSRP - gbm_pred)^2)

# Compare the MSEs
mse1
mse2
gbm_mse

```
Based on the Mean Squared Error (MSE) values of the three models, it seems that the second model (model2) performs the best on the test data, since it has the lowest MSE. Remember, MSE is a measure of the quality of an estimator or a predictor. It is always non-negative, and values closer to zero are better.

In comparison, the first model (model1) performs a bit worse than the second model, given its higher MSE.

The Gradient Boosting Model (gbm_model), however, performs the worst out of all three models, with a significantly higher MSE. This might indicate that the model is overfitting the training data, hence performing poorly on the unseen test data. It might also indicate that the hyperparameters of the gbm model need to be tuned to get a better result.

Remember, choosing the best model is not only about choosing the model with the lowest error metric (like MSE), but it's also important to consider the interpretability of the model, the trade-off between bias and variance, and the computational cost of the model.
